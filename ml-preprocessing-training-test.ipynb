{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "96a7a915-3792-478c-be13-5eba3fce1753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T23:17:09.397937Z",
     "iopub.status.busy": "2025-12-09T23:17:09.397653Z",
     "iopub.status.idle": "2025-12-09T23:27:17.649785Z",
     "shell.execute_reply": "2025-12-09T23:27:17.648480Z",
     "shell.execute_reply.started": "2025-12-09T23:17:09.397915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.ProcessingJob.NetworkConfig.VpcConfig.Subnets\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.ProcessingJob.NetworkConfig.VpcConfig.SecurityGroupIds\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.TrainingJob.VpcConfig.Subnets\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.TrainingJob.VpcConfig.SecurityGroupIds\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.TrainingJob.Environment\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.Model.VpcConfig\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "Training job: pipelines-9mrx8l9rgbs5-TrainingStep-CyzVHQ68aF\n",
      "s3_model_path: s3://feature-engineering-bucket-989220949c9c/models/pipelines-9mrx8l9rgbs5-TrainingStep-CyzVHQ68aF/output/model.tar.gz\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.Model.VpcConfig\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "------!"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> in &lt;module&gt;:155                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152 </span>)                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">153 </span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">154 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> predictor <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>155 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">RuntimeError</span><span style=\"font-weight: bold; text-decoration: underline\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"ğŸš¨ model.deploy() returned None â€” deployment likely failed.\"</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">156 </span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">157 </span>response = predictor.predict(<span style=\"color: #808000; text-decoration-color: #808000\">\"56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,2</span>   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">158 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(response)                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>ğŸš¨ <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-weight: bold\">model.deploy</span><span style=\"font-weight: bold\">()</span> returned <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span> â€” deployment likely failed.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0mâ•­â”€\u001b[0m\u001b[38;2;255;0;0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;255;0;0mâ”€â•®\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m in <module>:155                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m152 \u001b[0m)                                                                                          \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m153 \u001b[0m                                                                                           \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m154 \u001b[0m\u001b[94mif\u001b[0m predictor \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                                      \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m155 \u001b[2mâ”‚   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mRuntimeError\u001b[0m\u001b[1;4m(\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mğŸš¨ model.deploy() returned None â€” deployment likely failed.\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m)\u001b[0m      \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m156 \u001b[0m                                                                                           \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m157 \u001b[0mresponse = predictor.predict(\u001b[33m\"\u001b[0m\u001b[33m56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,2\u001b[0m   \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m158 \u001b[0m\u001b[96mprint\u001b[0m(response)                                                                            \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mğŸš¨ \u001b[1;38;2;225;0;225mmodel.deploy\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m returned \u001b[3;38;2;225;0;225mNone\u001b[0m â€” deployment likely failed.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/sagemaker-interactive-debugging": {
       "cell_id": "96a7a915-3792-478c-be13-5eba3fce1753",
       "debugging_info_folder": "/home/sagemaker-user/shared/.temp_sagemaker_unified_studio_debugging_info/96a7a915-3792-478c-be13-5eba3fce1753",
       "instruction_file": "/home/sagemaker-user/shared/.temp_sagemaker_unified_studio_debugging_info/ipython_debugging_sop.txt",
       "magic_command": "no_magic",
       "session_type": "python_3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# role = \"<sagemaker-execution-role>\"\n",
    "\n",
    "role=get_execution_role()\n",
    "bucket = \"feature-engineering-bucket-989220949c9c\"\n",
    "region = \"ap-south-1\"\n",
    "image_uri_train = \"961807745392.dkr.ecr.ap-south-1.amazonaws.com/mlops-train:latest\"\n",
    "\n",
    "# Preprocessing Step\n",
    "preprocess_processor = ScriptProcessor(\n",
    "    image_uri=image_uri_train,\n",
    "    role=role,\n",
    "    command=[\"python3\"],\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.medium\"\n",
    ")\n",
    "\n",
    "preprocess_step = ProcessingStep(\n",
    "    name=\"preprocess_step\",\n",
    "    processor=preprocess_processor,\n",
    "    code=\"preprocess.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=\"s3://feature-engineering-bucket-989220949c9c/Dataset/bank-additional-full.csv\",  # S3 input file\n",
    "            destination=\"/opt/ml/processing/input/\"  # Path inside the container\n",
    "        )\n",
    "        # {\"source\": \"s3://feature-engineering-bucket/bank-additional-full.csv\", \"destination\": \"/opt/ml/processing/input\"}\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"processed_data\",\n",
    "            source=\"/opt/ml/processing/output\",  # Path inside the container\n",
    "            destination=f\"s3://{bucket}/processed/\"  # S3 output location\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define Estimator\n",
    "estimator = Estimator(\n",
    "    image_uri=image_uri_train,\n",
    "    role=role,\n",
    "    entry_point=\"train.py\",             # âœ… Training script passed at runtime\n",
    "    source_dir=\".\",                     # âœ… Folder where train.py is located\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    base_job_name=\"mlops-train\",\n",
    "    output_path=f\"s3://{bucket}/models/\"  # âœ… where model.tar.gz will be saved\n",
    ")\n",
    "\n",
    "# Define TrainingStep in pipeline\n",
    "training_step = TrainingStep(\n",
    "    name=\"TrainingStep\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/processed/bank-additional-processed.csv\",\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Register model\n",
    "model_artifact = training_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "model = Model(\n",
    "    image_uri=image_uri_train,\n",
    "    model_data=model_artifact,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    model=model,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=\"mlops-model-group\",\n",
    "    approval_status=\"Approved\"\n",
    ")\n",
    "\n",
    "# Define Pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"MLOpsPipeline\",\n",
    "    steps=[preprocess_step, training_step, register_step]\n",
    ")\n",
    "\n",
    "# Deploy Pipeline\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "execution.wait()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. Deploy the Model to Endpoint\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "image_uri_inference = \"961807745392.dkr.ecr.ap-south-1.amazonaws.com/mlops-inference:latest\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "steps = execution.list_steps()\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "training_job_arn = None\n",
    "\n",
    "for step in steps:\n",
    "    if step[\"StepName\"] == \"TrainingStep\":\n",
    "        training_job_arn = step[\"Metadata\"][\"TrainingJob\"][\"Arn\"]\n",
    "        break\n",
    "\n",
    "# Extract training job name from ARN\n",
    "training_job_name = training_job_arn.split(\"/\")[-1]\n",
    "print(\"Training job:\", training_job_name)\n",
    "s3_model_path = f\"s3://{bucket}/models/{training_job_name}/output/model.tar.gz\"\n",
    "print(\"s3_model_path:\", s3_model_path)\n",
    "\n",
    "        \n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri_inference,\n",
    "    model_data=s3_model_path,\n",
    "    role=role,\n",
    "    # entry_point=\"serve.py\",       # âœ… Add this line\n",
    "    # source_dir=\".\",               # âœ… Or directory containing serve.py\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=\"mlops-prod-endpoint15\",\n",
    "    data_capture_config=DataCaptureConfig(\n",
    "        enable_capture=True,\n",
    "        sampling_percentage=100,\n",
    "        destination_s3_uri=f\"s3://{bucket}/monitoring/data/\",\n",
    "        capture_options=[\"Input\", \"Output\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "if predictor is None:\n",
    "    raise RuntimeError(\"ğŸš¨ model.deploy() returned None â€” deployment likely failed.\")\n",
    "\n",
    "response = predictor.predict(\"56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,261,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\")  # example input\n",
    "print(response)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 6. Monitor Performance Using Model Monitor\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "\n",
    "\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800\n",
    ")\n",
    "\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"mlops-monitor\",\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    output_s3_uri=f\"s3://{bucket}/monitoring/reports/\",\n",
    "    # generate the report every 5 minutes\n",
    "    schedule_cron_expression=\"cron(0/5 * * * ? *)\"\n",
    ")\n",
    "\n",
    "# Now performance reports will be generated daily in S3\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 7. Automate Retraining (Optional)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Use EventBridge or S3 trigger â†’ Lambda â†’ start pipeline\n",
    "# Lambda code (Python) can call:\n",
    "# import boto3\n",
    "# sm = boto3.client('sagemaker')\n",
    "# sm.start_pipeline_execution(PipelineName='MLOpsPipeline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_state": "idle",
   "id": "a4cb671b-6150-46a3-949b-2588df8eb330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T23:31:39.120098Z",
     "iopub.status.busy": "2025-12-09T23:31:39.119625Z",
     "iopub.status.idle": "2025-12-09T23:31:39.146795Z",
     "shell.execute_reply": "2025-12-09T23:31:39.145521Z",
     "shell.execute_reply.started": "2025-12-09T23:31:39.120063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> in &lt;module&gt;:2                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> predictor <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>2 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">RuntimeError</span><span style=\"font-weight: bold; text-decoration: underline\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"ğŸš¨ model.deploy() returned None â€” deployment likely failed.\"</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>response = predictor.predict(<span style=\"color: #808000; text-decoration-color: #808000\">\"56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,2</span>     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(response)                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">â”‚</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>ğŸš¨ <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-weight: bold\">model.deploy</span><span style=\"font-weight: bold\">()</span> returned <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span> â€” deployment likely failed.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0mâ•­â”€\u001b[0m\u001b[38;2;255;0;0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;255;0;0mâ”€â•®\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m in <module>:2                                                                                    \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m                                                                                                  \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m1 \u001b[0m\u001b[94mif\u001b[0m predictor \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                                        \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m2 \u001b[2mâ”‚   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mRuntimeError\u001b[0m\u001b[1;4m(\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mğŸš¨ model.deploy() returned None â€” deployment likely failed.\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m)\u001b[0m        \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m4 \u001b[0mresponse = predictor.predict(\u001b[33m\"\u001b[0m\u001b[33m56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,2\u001b[0m     \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ”‚\u001b[0m   \u001b[2m5 \u001b[0m\u001b[96mprint\u001b[0m(response)                                                                              \u001b[38;2;255;0;0mâ”‚\u001b[0m\n",
       "\u001b[38;2;255;0;0mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mğŸš¨ \u001b[1;38;2;225;0;225mmodel.deploy\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m returned \u001b[3;38;2;225;0;225mNone\u001b[0m â€” deployment likely failed.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/sagemaker-interactive-debugging": {
       "cell_id": "a4cb671b-6150-46a3-949b-2588df8eb330",
       "debugging_info_folder": "/home/sagemaker-user/shared/.temp_sagemaker_unified_studio_debugging_info/a4cb671b-6150-46a3-949b-2588df8eb330",
       "instruction_file": "/home/sagemaker-user/shared/.temp_sagemaker_unified_studio_debugging_info/ipython_debugging_sop.txt",
       "magic_command": "no_magic",
       "session_type": "python_3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if predictor is None:\n",
    "    raise RuntimeError(\"ğŸš¨ model.deploy() returned None â€” deployment likely failed.\")\n",
    "\n",
    "response = predictor.predict(\"56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,261,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\")  # example input\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "f7ff3068-14ed-4e2b-bccd-38764937be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_uri_inference = \"961807745392.dkr.ecr.ap-south-1.amazonaws.com/mlops-inference:latest\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "steps = execution.list_steps()\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "training_job_arn = None\n",
    "\n",
    "for step in steps:\n",
    "    if step[\"StepName\"] == \"TrainingStep\":\n",
    "        training_job_arn = step[\"Metadata\"][\"TrainingJob\"][\"Arn\"]\n",
    "        break\n",
    "\n",
    "# Extract training job name from ARN\n",
    "training_job_name = training_job_arn.split(\"/\")[-1]\n",
    "print(\"Training job:\", training_job_name)\n",
    "s3_model_path = f\"s3://{bucket}/models/{training_job_name}/output/model.tar.gz\"\n",
    "print(\"s3_model_path:\", s3_model_path)\n",
    "\n",
    "        \n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri_inference,\n",
    "    model_data=s3_model_path,\n",
    "    role=role,\n",
    "    entry_point=\"serve.py\",       # âœ… Add this line\n",
    "    source_dir=\".\",               # âœ… Or directory containing serve.py\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=\"mlops-prod-endpoint16\",\n",
    "    data_capture_config=DataCaptureConfig(\n",
    "        enable_capture=True,\n",
    "        sampling_percentage=100,\n",
    "        destination_s3_uri=f\"s3://{bucket}/monitoring/data/\",\n",
    "        capture_options=[\"Input\", \"Output\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "if predictor is None:\n",
    "    raise RuntimeError(\"ğŸš¨ model.deploy() returned None â€” deployment likely failed.\")\n",
    "\n",
    "response = predictor.predict(\"56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,261,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\")  # example input\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "13b6b7bf-cf93-4f5d-ae93-0f61df0180f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Step 1: Get the latest approved model package\n",
    "latest_model_package = sm.list_model_packages(\n",
    "    ModelPackageGroupName=\"mlops-model-group\",\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    "    SortBy=\"CreationTime\",\n",
    "    MaxResults=1\n",
    ")[\"ModelPackageSummaryList\"][0][\"ModelPackageArn\"]\n",
    "\n",
    "from sagemaker import ModelPackage\n",
    "mp = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=latest_model_package\n",
    ")\n",
    "\n",
    "predictor = mp.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=\"mlops-prod-endpoint19\"\n",
    ")\n",
    "\n",
    "if predictor is None:\n",
    "    raise RuntimeError(\"ğŸš¨ model.deploy() returned None â€” deployment likely failed.\")\n",
    "\n",
    "response = predictor.predict(\"56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,261,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\")  # example input\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f73fb1-6611-413a-85a9-523777a6298c",
   "metadata": {},
   "source": [
    "<h1>Pipeline where the inference uses the registry model instead of the trained model</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "bc2f67cc-643e-4a9f-a860-1a46456ed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# role = \"<sagemaker-execution-role>\"\n",
    "\n",
    "role=get_execution_role()\n",
    "bucket = \"feature-engineering-bucket-989220949c9c\"\n",
    "region = \"ap-south-1\"\n",
    "image_uri_train = \"961807745392.dkr.ecr.ap-south-1.amazonaws.com/mlops-train:latest\"\n",
    "image_uri_inference = \"961807745392.dkr.ecr.ap-south-1.amazonaws.com/mlops-inference:latest\"\n",
    "\n",
    "# Preprocessing Step\n",
    "preprocess_processor = ScriptProcessor(\n",
    "    image_uri=image_uri_train,\n",
    "    role=role,\n",
    "    command=[\"python3\"],\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.medium\"\n",
    ")\n",
    "\n",
    "preprocess_step = ProcessingStep(\n",
    "    name=\"preprocess_step\",\n",
    "    processor=preprocess_processor,\n",
    "    code=\"preprocess.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=\"s3://feature-engineering-bucket-989220949c9c/Dataset/bank-additional-full.csv\",  # S3 input file\n",
    "            destination=\"/opt/ml/processing/input/\"  # Path inside the container\n",
    "        )\n",
    "        # {\"source\": \"s3://feature-engineering-bucket/bank-additional-full.csv\", \"destination\": \"/opt/ml/processing/input\"}\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"processed_data\",\n",
    "            source=\"/opt/ml/processing/output\",  # Path inside the container\n",
    "            destination=f\"s3://{bucket}/processed/\"  # S3 output location\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define Estimator\n",
    "estimator = Estimator(\n",
    "    image_uri=image_uri_train,\n",
    "    role=role,\n",
    "    entry_point=\"train.py\",             # âœ… Training script passed at runtime\n",
    "    # source_dir=\".\",                     # âœ… Folder where train.py is located\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    base_job_name=\"mlops-train\",\n",
    "    output_path=f\"s3://{bucket}/models/\"  # âœ… where model.tar.gz will be saved\n",
    ")\n",
    "\n",
    "# Define TrainingStep in pipeline\n",
    "training_step = TrainingStep(\n",
    "    name=\"TrainingStep\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/processed/bank-additional-processed.csv\",\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Register model\n",
    "model_artifact = training_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "model = Model(\n",
    "    image_uri=image_uri_inference,\n",
    "    model_data=model_artifact,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    model=model,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=\"mlops-model-group\",\n",
    "    approval_status=\"Approved\"\n",
    ")\n",
    "\n",
    "# Define Pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"MLOpsPipeline\",\n",
    "    steps=[preprocess_step, training_step, register_step]\n",
    ")\n",
    "\n",
    "# Deploy Pipeline\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "execution.wait()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. Deploy the Model to Endpoint\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "steps = execution.list_steps()\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Step 1: Get the latest approved model package\n",
    "latest_model_package = sm.list_model_packages(\n",
    "    ModelPackageGroupName=\"mlops-model-group\",\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    "    SortBy=\"CreationTime\",\n",
    "    MaxResults=1\n",
    ")[\"ModelPackageSummaryList\"][0][\"ModelPackageArn\"]\n",
    "\n",
    "from sagemaker import ModelPackage\n",
    "mp = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=latest_model_package\n",
    ")\n",
    "\n",
    "\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "data_capture = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f\"s3://{bucket}/monitoring/data/\",\n",
    "    capture_options=[\"Input\", \"Output\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "predictor = mp.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=\"mlops-prod-endpoint-33\",\n",
    "    data_capture_config=data_capture\n",
    ")\n",
    "\n",
    "# If SDK returns None, manually create predictor\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.deserializers import StringDeserializer\n",
    "\n",
    "if predictor is None:\n",
    "    # predictor = Predictor(endpoint_name=\"mlops-prod-endpoint-20\", sagemaker_session=sagemaker_session)\n",
    "\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=\"mlops-prod-endpoint-33\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "        # deserializer=StringDeserializer(),\n",
    "    )\n",
    "\n",
    "\n",
    "response = predictor.predict(\"56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,261,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\")  # example input\n",
    "print(\"prediction results by the endpoint:\\n\")\n",
    "print(response)\n",
    "print(\"Using deserializer:\", type(predictor.deserializer))\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 6. Monitor Performance Using Model Monitor\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "\n",
    "\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800\n",
    ")\n",
    "\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"mlops-monitor\",\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    output_s3_uri=f\"s3://{bucket}/monitoring/reports/\",\n",
    "    # sagemaker does not support every 5 minutes, the minimum unit is hour\n",
    "    schedule_cron_expression=\"cron(0 * * * ? *)\"  # âœ” every hour\n",
    ")\n",
    "\n",
    "# Now performance reports will be generated daily in S3\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 7. Automate Retraining (Optional)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Use EventBridge or S3 trigger â†’ Lambda â†’ start pipeline\n",
    "# Lambda code (Python) can call:\n",
    "# import boto3\n",
    "# sm = boto3.client('sagemaker')\n",
    "# sm.start_pipeline_execution(PipelineName='MLOpsPipeline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f71f0-15bc-4f73-bee4-dc5f20b425d9",
   "metadata": {},
   "source": [
    "<h1>Monitoring part</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc765d6a-e270-4198-af5c-1094fedddaab",
   "metadata": {},
   "source": [
    "<h1>Deployment part</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "execution_state": "idle",
   "id": "25ef5a51-8391-429d-b487-7a4643febcef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:33:39.544083Z",
     "iopub.status.busy": "2025-12-10T12:33:39.543742Z",
     "iopub.status.idle": "2025-12-10T12:37:19.487830Z",
     "shell.execute_reply": "2025-12-10T12:37:19.486763Z",
     "shell.execute_reply.started": "2025-12-10T12:33:39.544060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.Model.VpcConfig\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "------!prediction results by the endpoint:\n",
      "\n",
      "[False]\n",
      "Using deserializer: <class 'sagemaker.base_deserializers.JSONDeserializer'>\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.MonitoringSchedule.MonitoringScheduleConfig.MonitoringJobDefinition.NetworkConfig.VpcConfig.Subnets\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.MonitoringSchedule.MonitoringScheduleConfig.MonitoringJobDefinition.NetworkConfig.VpcConfig.SecurityGroupIds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "steps = execution.list_steps()\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Step 1: Get the latest approved model package\n",
    "latest_model_package = sm.list_model_packages(\n",
    "    ModelPackageGroupName=\"mlops-model-group\",\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    "    SortBy=\"CreationTime\",\n",
    "    MaxResults=1\n",
    ")[\"ModelPackageSummaryList\"][0][\"ModelPackageArn\"]\n",
    "\n",
    "from sagemaker import ModelPackage\n",
    "mp = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=latest_model_package\n",
    ")\n",
    "\n",
    "\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "data_capture = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f\"s3://{bucket}/monitoring/data/\",\n",
    "    capture_options=[\"Input\", \"Output\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "predictor = mp.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=\"mlops-prod-endpoint-33\",\n",
    "    data_capture_config=data_capture\n",
    ")\n",
    "\n",
    "# If SDK returns None, manually create predictor\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.deserializers import StringDeserializer\n",
    "\n",
    "if predictor is None:\n",
    "    # predictor = Predictor(endpoint_name=\"mlops-prod-endpoint-20\", sagemaker_session=sagemaker_session)\n",
    "\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=\"mlops-prod-endpoint-33\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "        # deserializer=StringDeserializer(),\n",
    "    )\n",
    "\n",
    "\n",
    "response = predictor.predict(\"56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,261,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\")  # example input\n",
    "print(\"prediction results by the endpoint:\\n\")\n",
    "print(response)\n",
    "print(\"Using deserializer:\", type(predictor.deserializer))\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 6. Monitor Performance Using Model Monitor\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "\n",
    "\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800\n",
    ")\n",
    "\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"mlops-monitor\",\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    output_s3_uri=f\"s3://{bucket}/monitoring/reports/\",\n",
    "    # sagemaker does not support every 5 minutes, the minimum unit is hour\n",
    "    schedule_cron_expression=\"cron(0 * * * ? *)\"  # âœ” every hour\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "execution_state": "idle",
   "id": "d717549c-bb72-40cd-90f2-dddd198d1206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T11:49:36.563888Z",
     "iopub.status.busy": "2025-12-10T11:49:36.563604Z",
     "iopub.status.idle": "2025-12-10T11:49:36.711712Z",
     "shell.execute_reply": "2025-12-10T11:49:36.710652Z",
     "shell.execute_reply.started": "2025-12-10T11:49:36.563867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InService\n",
      "[{'VariantName': 'AllTraffic', 'DeployedImages': [{'SpecifiedImage': '961807745392.dkr.ecr.ap-south-1.amazonaws.com/mlops-inference@sha256:3b9259701464fea4c82d6bd679d296e05ef99f2fa7907882c00fa41279e613da', 'ResolvedImage': '961807745392.dkr.ecr.ap-south-1.amazonaws.com/mlops-inference@sha256:3b9259701464fea4c82d6bd679d296e05ef99f2fa7907882c00fa41279e613da', 'ResolutionTime': datetime.datetime(2025, 12, 10, 11, 33, 2, 729000, tzinfo=tzlocal())}], 'CurrentWeight': 1.0, 'DesiredWeight': 1.0, 'CurrentInstanceCount': 1, 'DesiredInstanceCount': 1}]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "desc = sm.describe_endpoint(EndpointName=\"mlops-prod-endpoint-29\")\n",
    "print(desc[\"EndpointStatus\"])\n",
    "print(desc[\"ProductionVariants\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "b1add65d-ac8b-48f7-8c26-509985388b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "execution.list_steps()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e54dbd-8abd-43f2-a89d-3aa2abfe8666",
   "metadata": {},
   "source": [
    "<h1>Delete monitoring</h1>\n",
    "To delete the monitoring, you can do the following operations:\n",
    "<h2>Option 1 â€” Delete the Monitoring Schedule (recommended)</h2>\n",
    "\n",
    "Effect:\n",
    "\n",
    "âœ” Stops monitoring\n",
    "\n",
    "âœ” Does not delete endpoint\n",
    "\n",
    "âœ” Safe and reversible (you can recreate later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33633bb2-06e9-4e94-a26f-e780a6d6ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "sm.delete_monitoring_schedule(\n",
    "    MonitoringScheduleName=\"mlops-monitor\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80d3b1-168f-46cf-8e7b-c80d3d02293f",
   "metadata": {},
   "source": [
    "<h2>Option 2 â€” Stop / Pause the Monitoring Schedule</h2>\n",
    "If you want to temporarily disable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb9d7b-9ccd-449a-8bc4-146ed2f816f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "sm.stop_monitoring_schedule(MonitoringScheduleName=\"mlops-monitor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5c4cf-73b5-4cff-8c4a-a1f3775780a9",
   "metadata": {},
   "source": [
    "Effect:\n",
    "\n",
    "âœ” Stops future monitoring jobs\n",
    "\n",
    "âœ” Schedule remains (can be restarted)\n",
    "\n",
    "To resume later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0e969-93ca-4bab-839d-6564bc3e976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.start_monitoring_schedule(MonitoringScheduleName=\"mlops-monitor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3d2c5-b262-48f7-bf48-13af709250fa",
   "metadata": {},
   "source": [
    "<h2>Option 3 â€” Delete the Endpoint (not required unless you want to)</h2>\n",
    "If you want to remove all cost from hosting the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c38c97-9281-41d2-987d-f7c4fc0ca028",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=\"mlops-prod-endpoint-30\")\n",
    "sm.delete_endpoint_config(EndpointConfigName=\"mlops-prod-endpoint-30\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16612040-ab74-491b-ac80-c2f029efd0f1",
   "metadata": {},
   "source": [
    "But note:\n",
    "\n",
    "ğŸš« This does NOT delete the monitoring schedule.\n",
    "    \n",
    "You must still run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b66bf-0276-48d3-a36c-2b154f0f4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_monitoring_schedule(MonitoringScheduleName=\"mlops-monitor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e19f08-b95b-44aa-bc2d-cfad132d8f04",
   "metadata": {},
   "source": [
    "Otherwise AWS will still try to run monitoring jobs â†’ errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a3012-c37f-4473-8880-4a0fea0621db",
   "metadata": {},
   "source": [
    "<h1>Check status of the endpoint via boto3</h1>\n",
    "If you see:\n",
    "\n",
    "- Status: Deleting â†’ you must wait until deletion finishes.\n",
    "- Status: Failed or anything else â†’ force delete or use a new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a7446-262f-434e-9547-836cfb8389e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "endpoint_name = 'mlops-prod-endpoint3'\n",
    "\n",
    "try:\n",
    "    response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(\"Status:\", response['EndpointStatus'])\n",
    "except sm.exceptions.ClientError as e:\n",
    "    if \"Could not find\" in str(e):\n",
    "        print(\"âœ… Endpoint does not exist\")\n",
    "    else:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a07d86-84a0-4697-ae03-c6e80cd0ee7c",
   "metadata": {},
   "source": [
    "<h1>Add the automate retraining</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cd4b5-5125-4f42-a70c-0a6d0b193cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# role = \"<sagemaker-execution-role>\"\n",
    "\n",
    "role=get_execution_role()\n",
    "bucket = \"feature-engineering-bucket-989220949c9c\"\n",
    "region = \"ap-south-1\"\n",
    "image_uri_train = \"961807745392.dkr.ecr.ap-south-1.amazonaws.com/mlops-train:latest\"\n",
    "image_uri_inference = \"961807745392.dkr.ecr.ap-south-1.amazonaws.com/mlops-inference:latest\"\n",
    "\n",
    "# Preprocessing Step\n",
    "preprocess_processor = ScriptProcessor(\n",
    "    image_uri=image_uri_train,\n",
    "    role=role,\n",
    "    command=[\"python3\"],\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.medium\"\n",
    ")\n",
    "\n",
    "preprocess_step = ProcessingStep(\n",
    "    name=\"preprocess_step\",\n",
    "    processor=preprocess_processor,\n",
    "    code=\"preprocess.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=\"s3://feature-engineering-bucket-989220949c9c/Dataset/bank-additional-full.csv\",  # S3 input file\n",
    "            destination=\"/opt/ml/processing/input/\"  # Path inside the container\n",
    "        )\n",
    "        # {\"source\": \"s3://feature-engineering-bucket/bank-additional-full.csv\", \"destination\": \"/opt/ml/processing/input\"}\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"processed_data\",\n",
    "            source=\"/opt/ml/processing/output\",  # Path inside the container\n",
    "            destination=f\"s3://{bucket}/processed/\"  # S3 output location\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define Estimator\n",
    "estimator = Estimator(\n",
    "    image_uri=image_uri_train,\n",
    "    role=role,\n",
    "    entry_point=\"train.py\",             # âœ… Training script passed at runtime\n",
    "    # source_dir=\".\",                     # âœ… Folder where train.py is located\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    base_job_name=\"mlops-train\",\n",
    "    output_path=f\"s3://{bucket}/models/\"  # âœ… where model.tar.gz will be saved\n",
    ")\n",
    "\n",
    "# Define TrainingStep in pipeline\n",
    "training_step = TrainingStep(\n",
    "    name=\"TrainingStep\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/processed/bank-additional-processed.csv\",\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Register model\n",
    "model_artifact = training_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "model = Model(\n",
    "    image_uri=image_uri_inference,\n",
    "    model_data=model_artifact,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    model=model,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=\"mlops-model-group\",\n",
    "    approval_status=\"Approved\"\n",
    ")\n",
    "\n",
    "# Define Pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"MLOpsPipeline\",\n",
    "    steps=[preprocess_step, training_step, register_step]\n",
    ")\n",
    "\n",
    "# Deploy Pipeline\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "execution.wait()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 5. Deploy the Model to Endpoint\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "steps = execution.list_steps()\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Step 1: Get the latest approved model package\n",
    "latest_model_package = sm.list_model_packages(\n",
    "    ModelPackageGroupName=\"mlops-model-group\",\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    "    SortBy=\"CreationTime\",\n",
    "    MaxResults=1\n",
    ")[\"ModelPackageSummaryList\"][0][\"ModelPackageArn\"]\n",
    "\n",
    "from sagemaker import ModelPackage\n",
    "mp = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=latest_model_package\n",
    ")\n",
    "\n",
    "\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "data_capture = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f\"s3://{bucket}/monitoring/data/\",\n",
    "    capture_options=[\"Input\", \"Output\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "predictor = mp.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=\"mlops-prod-endpoint-33\",\n",
    "    data_capture_config=data_capture\n",
    ")\n",
    "\n",
    "# If SDK returns None, manually create predictor\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.deserializers import StringDeserializer\n",
    "\n",
    "if predictor is None:\n",
    "    # predictor = Predictor(endpoint_name=\"mlops-prod-endpoint-20\", sagemaker_session=sagemaker_session)\n",
    "\n",
    "    predictor = Predictor(\n",
    "        endpoint_name=\"mlops-prod-endpoint-33\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        serializer=CSVSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "        # deserializer=StringDeserializer(),\n",
    "    )\n",
    "\n",
    "\n",
    "response = predictor.predict(\"56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,261,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0\")  # example input\n",
    "print(\"prediction results by the endpoint:\\n\")\n",
    "print(response)\n",
    "print(\"Using deserializer:\", type(predictor.deserializer))\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 6. Monitor Performance Using Model Monitor\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "\n",
    "\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800\n",
    ")\n",
    "\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"mlops-monitor\",\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    output_s3_uri=f\"s3://{bucket}/monitoring/reports/\",\n",
    "    # sagemaker does not support every 5 minutes, the minimum unit is hour\n",
    "    schedule_cron_expression=\"cron(0 * * * ? *)\"  # âœ” every hour\n",
    ")\n",
    "\n",
    "# Now performance reports will be generated daily in S3\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 7. Automate Retraining (Optional)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Use EventBridge or S3 trigger â†’ Lambda â†’ start pipeline\n",
    "# Lambda code (Python) can call:\n",
    "# import boto3\n",
    "# sm = boto3.client('sagemaker')\n",
    "# sm.start_pipeline_execution(PipelineName='MLOpsPipeline')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
